"""
Ollama IDE - GPT ê°™ì€ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
í„°ë¯¸ë„ ê¸°ë°˜ ëŒ€í™”í˜• AI ì±—ë´‡
"""

import os
import json
import requests
from datetime import datetime
from pathlib import Path

# ì„¤ì •
OLLAMA_URL = "http://localhost:11434"
HISTORY_DIR = "data/ollama_chat"
os.makedirs(HISTORY_DIR, exist_ok=True)

# ìƒ‰ìƒ ì½”ë“œ
class Color:
    RESET = '\033[0m'
    BOLD = '\033[1m'
    GREEN = '\033[92m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    GRAY = '\033[90m'


def print_colored(text, color=Color.RESET):
    print(f"{color}{text}{Color.RESET}")


def get_available_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ Ollama ëª¨ë¸ ì¡°íšŒ"""
    try:
        response = requests.get(f"{OLLAMA_URL}/api/tags", timeout=5)
        response.raise_for_status()
        data = response.json()
        models = [m['name'] for m in data.get('models', [])]
        return models
    except Exception as e:
        print_colored(f"âŒ Ollama ì—°ê²° ì‹¤íŒ¨: {e}", Color.RED)
        print_colored("   Ollama ì•±ì´ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”.", Color.GRAY)
        return []


def select_model(models):
    """ëª¨ë¸ ì„ íƒ"""
    if not models:
        return None
    
    print_colored("\nì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:", Color.CYAN)
    for i, model in enumerate(models, 1):
        print(f"  {i}. {model}")
    
    while True:
        try:
            choice = input(f"\nëª¨ë¸ ì„ íƒ (1-{len(models)}) [ê¸°ë³¸: 1]: ").strip()
            if not choice:
                return models[0]
            idx = int(choice) - 1
            if 0 <= idx < len(models):
                return models[idx]
            print_colored("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.", Color.RED)
        except (ValueError, KeyboardInterrupt):
            return models[0]


def chat_with_ollama(model, prompt, stream=False):
    """Ollama API í˜¸ì¶œ"""
    try:
        response = requests.post(
            f"{OLLAMA_URL}/api/generate",
            json={
                "model": model,
                "prompt": prompt,
                "stream": stream
            },
            timeout=120
        )
        response.raise_for_status()
        data = response.json()
        return data.get('response', ''), None
    except Exception as e:
        return None, str(e)


def save_chat_history(model, history):
    """ì±„íŒ… íˆìŠ¤í† ë¦¬ ì €ì¥"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = os.path.join(HISTORY_DIR, f"chat_{timestamp}.json")
    
    data = {
        "model": model,
        "timestamp": datetime.now().isoformat(),
        "messages": history
    }
    
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    return filename


def load_recent_chats(limit=5):
    """ìµœê·¼ ëŒ€í™” ë¶ˆëŸ¬ì˜¤ê¸°"""
    files = list(Path(HISTORY_DIR).glob("chat_*.json"))
    files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
    
    chats = []
    for file in files[:limit]:
        try:
            with open(file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                chats.append({
                    'file': file.name,
                    'timestamp': data.get('timestamp', ''),
                    'model': data.get('model', ''),
                    'message_count': len(data.get('messages', []))
                })
        except Exception:
            continue
    
    return chats


def print_banner():
    """ì‹œì‘ ë°°ë„ˆ ì¶œë ¥"""
    print_colored("\n" + "="*60, Color.CYAN)
    print_colored("  ğŸ¤– Ollama IDE - GPT ê°™ì€ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤", Color.BOLD + Color.GREEN)
    print_colored("="*60, Color.CYAN)
    print_colored("  ëª…ë ¹ì–´:", Color.YELLOW)
    print_colored("    /exit   - ì¢…ë£Œ", Color.GRAY)
    print_colored("    /clear  - í™”ë©´ ì§€ìš°ê¸°", Color.GRAY)
    print_colored("    /save   - ëŒ€í™” ì €ì¥", Color.GRAY)
    print_colored("    /model  - ëª¨ë¸ ë³€ê²½", Color.GRAY)
    print_colored("    /history - ìµœê·¼ ëŒ€í™” ë³´ê¸°", Color.GRAY)
    print_colored("="*60 + "\n", Color.CYAN)


def print_message(role, content, color):
    """ë©”ì‹œì§€ ì¶œë ¥"""
    prefix = "ğŸ‘¤ You" if role == "user" else "ğŸ¤– AI"
    print_colored(f"\n{prefix}:", color + Color.BOLD)
    print_colored(content, color)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print_banner()
    
    # ëª¨ë¸ ì„ íƒ
    models = get_available_models()
    if not models:
        print_colored("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.", Color.RED)
        return
    
    current_model = select_model(models)
    print_colored(f"\nâœ… ì„ íƒëœ ëª¨ë¸: {current_model}\n", Color.GREEN)
    
    # ìµœê·¼ ëŒ€í™” í‘œì‹œ
    recent = load_recent_chats(3)
    if recent:
        print_colored("ìµœê·¼ ëŒ€í™”:", Color.CYAN)
        for chat in recent:
            print(f"  â€¢ {chat['file']} - {chat['message_count']}ê°œ ë©”ì‹œì§€ ({chat['model']})")
        print()
    
    # ëŒ€í™” ì‹œì‘
    history = []
    
    while True:
        try:
            # ì‚¬ìš©ì ì…ë ¥
            user_input = input(f"{Color.BLUE}You > {Color.RESET}").strip()
            
            if not user_input:
                continue
            
            # ëª…ë ¹ì–´ ì²˜ë¦¬
            if user_input.startswith('/'):
                cmd = user_input.lower()
                
                if cmd == '/exit' or cmd == '/quit':
                    if history:
                        save_path = save_chat_history(current_model, history)
                        print_colored(f"\nğŸ’¾ ëŒ€í™” ì €ì¥ë¨: {save_path}", Color.GREEN)
                    print_colored("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.\n", Color.CYAN)
                    break
                
                elif cmd == '/clear':
                    os.system('cls' if os.name == 'nt' else 'clear')
                    print_banner()
                    continue
                
                elif cmd == '/save':
                    save_path = save_chat_history(current_model, history)
                    print_colored(f"ğŸ’¾ ì €ì¥ë¨: {save_path}", Color.GREEN)
                    continue
                
                elif cmd == '/model':
                    models = get_available_models()
                    current_model = select_model(models)
                    print_colored(f"âœ… ëª¨ë¸ ë³€ê²½: {current_model}", Color.GREEN)
                    continue
                
                elif cmd == '/history':
                    recent = load_recent_chats(5)
                    if recent:
                        print_colored("\nìµœê·¼ ëŒ€í™”:", Color.CYAN)
                        for i, chat in enumerate(recent, 1):
                            print(f"  {i}. {chat['file']} - {chat['message_count']}ê°œ ({chat['model']})")
                    else:
                        print_colored("ì €ì¥ëœ ëŒ€í™”ê°€ ì—†ìŠµë‹ˆë‹¤.", Color.GRAY)
                    continue
                
                else:
                    print_colored("ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹ì–´ì…ë‹ˆë‹¤.", Color.RED)
                    continue
            
            # ì‚¬ìš©ì ë©”ì‹œì§€ ì €ì¥
            history.append({"role": "user", "content": user_input})
            
            # AI ì‘ë‹µ ìƒì„±
            print_colored(f"\n{Color.GRAY}ìƒì„± ì¤‘...{Color.RESET}", Color.GRAY)
            response, error = chat_with_ollama(current_model, user_input)
            
            if error:
                print_colored(f"âŒ ì˜¤ë¥˜: {error}", Color.RED)
                history.pop()  # ì‹¤íŒ¨í•œ ë©”ì‹œì§€ ì œê±°
                continue
            
            if not response:
                print_colored("âŒ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", Color.RED)
                history.pop()
                continue
            
            # AI ì‘ë‹µ ì €ì¥ ë° ì¶œë ¥
            history.append({"role": "assistant", "content": response})
            print_message("assistant", response, Color.GREEN)
            
        except KeyboardInterrupt:
            print_colored("\n\nâš ï¸  Ctrl+C ê°ì§€ë¨. ì¢…ë£Œí•˜ë ¤ë©´ /exit ì…ë ¥", Color.YELLOW)
            continue
        except Exception as e:
            print_colored(f"\nâŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", Color.RED)
            continue
    
    print()


if __name__ == "__main__":
    main()
