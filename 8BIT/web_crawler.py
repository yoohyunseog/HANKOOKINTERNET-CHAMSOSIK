"""
ì›¹ ë°ì´í„° í¬ë¡¤ëŸ¬
Seleniumì„ ì‚¬ìš©í•˜ì—¬ ì›¹í˜ì´ì§€ì—ì„œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
"""

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import json
import time
from datetime import datetime
from urllib.parse import urlparse

class WebCrawler:
    def __init__(self, headless=True):
        """
        ì›¹ í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”
        
        Args:
            headless (bool): ë¸Œë¼ìš°ì €ë¥¼ ìˆ¨ê¹€ ëª¨ë“œë¡œ ì‹¤í–‰í• ì§€ ì—¬ë¶€
        """
        self.setup_driver(headless)
        self.collected_data = []
        
    def setup_driver(self, headless):
        """Chrome WebDriver ì„¤ì •"""
        chrome_options = Options()
        
        if headless:
            chrome_options.add_argument('--headless')
        
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        self.driver = webdriver.Chrome(options=chrome_options)
        self.wait = WebDriverWait(self.driver, 10)
        
    def crawl_page(self, url):
        """
        ë‹¨ì¼ í˜ì´ì§€ í¬ë¡¤ë§
        
        Args:
            url (str): í¬ë¡¤ë§í•  URL
            
        Returns:
            dict: í¬ë¡¤ë§ëœ ë°ì´í„°
        """
        try:
            print(f"ğŸ“¥ í¬ë¡¤ë§ ì‹œì‘: {url}")
            
            # í˜ì´ì§€ ë¡œë“œ
            self.driver.get(url)
            time.sleep(2)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            
            # HTML ê°€ì ¸ì˜¤ê¸°
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            # ë°ì´í„° ì¶”ì¶œ
            data = {
                'url': url,
                'domain': urlparse(url).netloc,
                'title': self.extract_title(soup),
                'meta_description': self.extract_meta_description(soup),
                'headings': self.extract_headings(soup),
                'paragraphs': self.extract_paragraphs(soup),
                'links': self.extract_links(soup, url),
                'images': self.extract_images(soup),
                'crawled_at': datetime.now().isoformat(),
            }
            
            print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {data['title']}")
            return data
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {url}")
            print(f"   ì˜¤ë¥˜: {str(e)}")
            return None
    
    def extract_title(self, soup):
        """í˜ì´ì§€ ì œëª© ì¶”ì¶œ"""
        title_tag = soup.find('title')
        if title_tag:
            return title_tag.get_text().strip()
        
        h1_tag = soup.find('h1')
        if h1_tag:
            return h1_tag.get_text().strip()
        
        return "ì œëª© ì—†ìŒ"
    
    def extract_meta_description(self, soup):
        """ë©”íƒ€ ì„¤ëª… ì¶”ì¶œ"""
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            return meta_desc['content'].strip()
        
        meta_desc = soup.find('meta', attrs={'property': 'og:description'})
        if meta_desc and meta_desc.get('content'):
            return meta_desc['content'].strip()
        
        return ""
    
    def extract_headings(self, soup):
        """ì œëª© íƒœê·¸(h1-h6) ì¶”ì¶œ"""
        headings = []
        for i in range(1, 7):
            for heading in soup.find_all(f'h{i}'):
                text = heading.get_text().strip()
                if text:
                    headings.append({
                        'level': i,
                        'text': text
                    })
        return headings[:20]  # ìµœëŒ€ 20ê°œ
    
    def extract_paragraphs(self, soup):
        """ë³¸ë¬¸ ë‹¨ë½ ì¶”ì¶œ"""
        paragraphs = []
        for p in soup.find_all('p'):
            text = p.get_text().strip()
            if text and len(text) > 20:  # 20ì ì´ìƒë§Œ
                paragraphs.append(text)
        return paragraphs[:50]  # ìµœëŒ€ 50ê°œ
    
    def extract_links(self, soup, base_url):
        """ë§í¬ ì¶”ì¶œ"""
        links = []
        for a in soup.find_all('a', href=True):
            href = a['href']
            text = a.get_text().strip()
            
            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
            if href.startswith('/'):
                parsed = urlparse(base_url)
                href = f"{parsed.scheme}://{parsed.netloc}{href}"
            
            if href.startswith('http') and text:
                links.append({
                    'url': href,
                    'text': text
                })
        
        return links[:30]  # ìµœëŒ€ 30ê°œ
    
    def extract_images(self, soup):
        """ì´ë¯¸ì§€ ì¶”ì¶œ"""
        images = []
        for img in soup.find_all('img', src=True):
            alt = img.get('alt', '').strip()
            src = img['src']
            
            images.append({
                'src': src,
                'alt': alt
            })
        
        return images[:20]  # ìµœëŒ€ 20ê°œ
    
    def crawl_multiple(self, urls):
        """
        ì—¬ëŸ¬ í˜ì´ì§€ í¬ë¡¤ë§
        
        Args:
            urls (list): URL ë¦¬ìŠ¤íŠ¸
            
        Returns:
            list: í¬ë¡¤ë§ëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{len(urls)}] í¬ë¡¤ë§ ì¤‘...")
            
            data = self.crawl_page(url)
            if data:
                results.append(data)
                self.collected_data.append(data)
            
            # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´
            time.sleep(1)
        
        return results
    
    def save_to_json(self, filename='crawled_data.json'):
        """
        ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            filename (str): ì €ì¥í•  íŒŒì¼ëª…
        """
        output = {
            'total_count': len(self.collected_data),
            'crawled_at': datetime.now().isoformat(),
            'data': self.collected_data
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename}")
        print(f"   ì´ {len(self.collected_data)}ê°œ í˜ì´ì§€ ìˆ˜ì§‘")
    
    def close(self):
        """ë¸Œë¼ìš°ì € ë‹«ê¸°"""
        if self.driver:
            self.driver.quit()
            print("\nğŸ”’ ë¸Œë¼ìš°ì € ì¢…ë£Œ")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # í¬ë¡¤ë§í•  URL ë¦¬ìŠ¤íŠ¸
    urls = [
        'https://www.naver.com',
        'https://www.python.org',
        # ì—¬ê¸°ì— í¬ë¡¤ë§í•  URL ì¶”ê°€
    ]
    
    print("=" * 60)
    print("ğŸ¤– ì›¹ í¬ë¡¤ëŸ¬ ì‹œì‘")
    print("=" * 60)
    
    crawler = WebCrawler(headless=False)  # ë¸Œë¼ìš°ì € í‘œì‹œ
    
    try:
        # í¬ë¡¤ë§ ì‹¤í–‰
        crawler.crawl_multiple(urls)
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        crawler.save_to_json('data/crawled_data.json')
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "=" * 60)
        print("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
        print("=" * 60)
        
        for i, data in enumerate(crawler.collected_data, 1):
            print(f"{i}. {data['title']}")
            print(f"   URL: {data['url']}")
            print(f"   ë‹¨ë½ ìˆ˜: {len(data['paragraphs'])}")
            print(f"   ë§í¬ ìˆ˜: {len(data['links'])}")
            print()
        
    finally:
        crawler.close()


if __name__ == '__main__':
    main()
