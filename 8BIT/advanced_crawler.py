"""
í†µí•© ì›¹ ë°ì´í„° í¬ë¡¤ëŸ¬
í¬ë¡¤ë§ + ì¥ë¥´ë¶„ë¥˜ + ìš”ì•½ + í‚¤ì›Œë“œì¶”ì¶œì„ í•œ ë²ˆì— ìˆ˜í–‰í•©ë‹ˆë‹¤.
"""

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from bs4 import BeautifulSoup
import json
import time
import os
from datetime import datetime
from urllib.parse import urlparse

from genre_classifier import GenreClassifier
from text_summarizer import TextSummarizer
from keyword_extractor import KeywordExtractor


class AdvancedWebCrawler:
    def __init__(self, headless=True):
        """í†µí•© í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”"""
        self.setup_driver(headless)
        self.collected_data = []
        
        # ê° ëª¨ë“ˆ ì´ˆê¸°í™”
        self.classifier = GenreClassifier()
        self.summarizer = TextSummarizer()
        self.extractor = KeywordExtractor()
        
        print("âœ… í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ")
        print("   - ì¥ë¥´ ë¶„ë¥˜ê¸°: ì¤€ë¹„")
        print("   - í…ìŠ¤íŠ¸ ìš”ì•½ê¸°: ì¤€ë¹„")
        print("   - í‚¤ì›Œë“œ ì¶”ì¶œê¸°: ì¤€ë¹„")
    
    def setup_driver(self, headless):
        """Chrome WebDriver ì„¤ì •"""
        chrome_options = Options()
        
        if headless:
            chrome_options.add_argument('--headless')
        
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        self.driver = webdriver.Chrome(options=chrome_options)
        self.wait = WebDriverWait(self.driver, 10)
    
    def crawl_page(self, url):
        """
        í˜ì´ì§€ í¬ë¡¤ë§ + ë¶„ì„ í†µí•© ì‹¤í–‰
        
        Args:
            url (str): í¬ë¡¤ë§í•  URL
            
        Returns:
            dict: ì™„ì „íˆ ì²˜ë¦¬ëœ ë°ì´í„°
        """
        try:
            print(f"\n{'='*60}")
            print(f"ğŸ“¥ í¬ë¡¤ë§: {url}")
            
            # 1. í˜ì´ì§€ ë¡œë“œ
            self.driver.get(url)
            time.sleep(2)
            
            # 2. HTML íŒŒì‹±
            html = self.driver.page_source
            soup = BeautifulSoup(html, 'html.parser')
            
            # 3. ê¸°ë³¸ ë°ì´í„° ì¶”ì¶œ
            title = self.extract_title(soup)
            print(f"   ì œëª©: {title}")
            
            meta_desc = self.extract_meta_description(soup)
            paragraphs = self.extract_paragraphs(soup)
            
            # ë³¸ë¬¸ í…ìŠ¤íŠ¸ ê²°í•©
            content = ' '.join(paragraphs[:20])  # ì•ë¶€ë¶„ 20ê°œ ë‹¨ë½ë§Œ
            
            # 4. ì¥ë¥´ ë¶„ë¥˜
            genre = self.classifier.classify(url, title, content)
            print(f"   ì¥ë¥´: {genre}")
            
            # 5. 1ì¤„ ìš”ì•½
            summary = self.summarizer.summarize(content, title, max_length=100)
            print(f"   ìš”ì•½: {summary}")
            
            # 6. í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self.extractor.extract_keywords(content, title, max_keywords=7)
            print(f"   í‚¤ì›Œë“œ: {', '.join(keywords)}")
            
            # 7. ìµœì¢… ë°ì´í„° êµ¬ì„±
            data = {
                'id': self.generate_id(),
                'url': url,
                'domain': urlparse(url).netloc,
                'title': title,
                'genre': genre,
                'summary': summary,
                'keywords': keywords,
                'meta_description': meta_desc,
                'paragraphs_count': len(paragraphs),
                'content_length': len(content),
                'crawled_at': datetime.now().isoformat(),
            }
            
            print(f"âœ… ì™„ë£Œ")
            return data
            
        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {url}")
            print(f"   ì˜¤ë¥˜: {str(e)}")
            return None
    
    def generate_id(self):
        """ê³ ìœ  ID ìƒì„±"""
        timestamp = datetime.now().strftime('%Y%m%d%H%M%S')
        return f"crawl_{timestamp}_{len(self.collected_data) + 1}"
    
    def extract_title(self, soup):
        """í˜ì´ì§€ ì œëª© ì¶”ì¶œ"""
        # 1. <title> íƒœê·¸
        title_tag = soup.find('title')
        if title_tag:
            return title_tag.get_text().strip()
        
        # 2. <h1> íƒœê·¸
        h1_tag = soup.find('h1')
        if h1_tag:
            return h1_tag.get_text().strip()
        
        # 3. og:title ë©”íƒ€ íƒœê·¸
        og_title = soup.find('meta', attrs={'property': 'og:title'})
        if og_title and og_title.get('content'):
            return og_title['content'].strip()
        
        return "ì œëª© ì—†ìŒ"
    
    def extract_meta_description(self, soup):
        """ë©”íƒ€ ì„¤ëª… ì¶”ì¶œ"""
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and meta_desc.get('content'):
            return meta_desc['content'].strip()
        
        og_desc = soup.find('meta', attrs={'property': 'og:description'})
        if og_desc and og_desc.get('content'):
            return og_desc['content'].strip()
        
        return ""
    
    def extract_paragraphs(self, soup):
        """ë³¸ë¬¸ ë‹¨ë½ ì¶”ì¶œ"""
        paragraphs = []
        
        for p in soup.find_all('p'):
            text = p.get_text().strip()
            if text and len(text) > 20:
                paragraphs.append(text)
        
        return paragraphs
    
    def crawl_multiple(self, urls, delay=2):
        """
        ì—¬ëŸ¬ í˜ì´ì§€ í¬ë¡¤ë§
        
        Args:
            urls (list): URL ë¦¬ìŠ¤íŠ¸
            delay (int): í˜ì´ì§€ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
            
        Returns:
            list: í¬ë¡¤ë§ëœ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        """
        results = []
        total = len(urls)
        
        print("\n" + "="*60)
        print(f"ğŸš€ í¬ë¡¤ë§ ì‹œì‘: ì´ {total}ê°œ í˜ì´ì§€")
        print("="*60)
        
        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{total}] ì§„í–‰ ì¤‘...")
            
            data = self.crawl_page(url)
            if data:
                results.append(data)
                self.collected_data.append(data)
            
            # ì„œë²„ ë¶€í•˜ ë°©ì§€
            if i < total:
                time.sleep(delay)
        
        print("\n" + "="*60)
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(results)}ê°œ ì„±ê³µ")
        print("="*60)
        
        return results
    
    def save_to_json(self, filename=None):
        """
        ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            filename (str): ì €ì¥í•  íŒŒì¼ëª… (ê¸°ë³¸ê°’: ìë™ ìƒì„±)
        """
        if not self.collected_data:
            print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # íŒŒì¼ëª… ìë™ ìƒì„±
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'data/crawled_data_{timestamp}.json'
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        # ë°ì´í„° êµ¬ì„±
        output = {
            'total_count': len(self.collected_data),
            'crawled_at': datetime.now().isoformat(),
            'stats': self.get_stats(),
            'data': self.collected_data
        }
        
        # JSON ì €ì¥
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ")
        print(f"   íŒŒì¼: {filename}")
        print(f"   ê°œìˆ˜: {len(self.collected_data)}ê°œ")
        print(f"   í¬ê¸°: {os.path.getsize(filename):,} bytes")
    
    def get_stats(self):
        """í†µê³„ ì •ë³´ ìƒì„±"""
        if not self.collected_data:
            return {}
        
        # ì¥ë¥´ë³„ ê°œìˆ˜
        genre_counts = {}
        total_keywords = 0
        
        for item in self.collected_data:
            genre = item.get('genre', 'ê¸°íƒ€')
            genre_counts[genre] = genre_counts.get(genre, 0) + 1
            total_keywords += len(item.get('keywords', []))
        
        return {
            'total_pages': len(self.collected_data),
            'total_keywords': total_keywords,
            'avg_keywords_per_page': round(total_keywords / len(self.collected_data), 1),
            'genre_distribution': genre_counts,
        }
    
    def print_summary(self):
        """ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        if not self.collected_data:
            print("\nìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        stats = self.get_stats()
        
        print("\n" + "="*60)
        print("ğŸ“Š í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        print(f"\nğŸ“„ ì´ í˜ì´ì§€: {stats['total_pages']}ê°œ")
        print(f"ğŸ” ì´ í‚¤ì›Œë“œ: {stats['total_keywords']}ê°œ")
        print(f"ğŸ“ˆ í˜ì´ì§€ë‹¹ í‰ê·  í‚¤ì›Œë“œ: {stats['avg_keywords_per_page']}ê°œ")
        
        print(f"\nğŸ¯ ì¥ë¥´ë³„ ë¶„í¬:")
        for genre, count in stats['genre_distribution'].items():
            percentage = (count / stats['total_pages']) * 100
            print(f"   {genre}: {count}ê°œ ({percentage:.1f}%)")
        
        print(f"\nğŸ“‹ ìˆ˜ì§‘ëœ ë°ì´í„° ëª©ë¡:")
        for i, item in enumerate(self.collected_data, 1):
            print(f"\n{i}. [{item['genre']}] {item['title']}")
            print(f"   URL: {item['url']}")
            print(f"   ìš”ì•½: {item['summary']}")
            print(f"   í‚¤ì›Œë“œ: {', '.join(item['keywords'])}")
    
    def close(self):
        """ë¸Œë¼ìš°ì € ë‹«ê¸°"""
        if self.driver:
            self.driver.quit()
            print("\nğŸ”’ ë¸Œë¼ìš°ì € ì¢…ë£Œ")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # í¬ë¡¤ë§í•  URL ë¦¬ìŠ¤íŠ¸ (ì˜ˆì œ)
    urls = [
        'https://www.python.org',
        'https://github.com/trending',
        # ì—¬ê¸°ì— í¬ë¡¤ë§í•  URL ì¶”ê°€
    ]
    
    print("=" * 60)
    print("ğŸ¤– í†µí•© ì›¹ ë°ì´í„° í¬ë¡¤ëŸ¬")
    print("   - í¬ë¡¤ë§, ì¥ë¥´ë¶„ë¥˜, ìš”ì•½, í‚¤ì›Œë“œì¶”ì¶œ")
    print("=" * 60)
    
    crawler = AdvancedWebCrawler(headless=False)
    
    try:
        # í¬ë¡¤ë§ ì‹¤í–‰
        crawler.crawl_multiple(urls, delay=2)
        
        # ê²°ê³¼ ì¶œë ¥
        crawler.print_summary()
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        crawler.save_to_json()
        
    finally:
        crawler.close()


if __name__ == '__main__':
    main()
