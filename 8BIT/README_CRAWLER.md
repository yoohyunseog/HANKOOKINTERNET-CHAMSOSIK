# ğŸ¤– ì›¹ ë°ì´í„° í¬ë¡¤ëŸ¬ ì‚¬ìš© ê°€ì´ë“œ

ì™„ì „í•œ ì›¹ ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ: í¬ë¡¤ë§ + ì¥ë¥´ë¶„ë¥˜ + ìš”ì•½ + í‚¤ì›Œë“œì¶”ì¶œ

## ğŸ“¦ ì„¤ì¹˜

### 1. ê°€ìƒí™˜ê²½ í™œì„±í™”
```powershell
# Windows PowerShell
.\.venv\Scripts\activate
```

### 2. íŒ¨í‚¤ì§€ ì„¤ì¹˜
```powershell
pip install -r 8BIT/requirements_crawler.txt
```

### 3. Chrome WebDriver ì„¤ì¹˜
Chrome ë¸Œë¼ìš°ì €ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
Seleniumì´ ìë™ìœ¼ë¡œ ChromeDriverë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### í†µí•© í¬ë¡¤ëŸ¬ (ì¶”ì²œ)
```python
from advanced_crawler import AdvancedWebCrawler

# í¬ë¡¤ë§í•  URL ë¦¬ìŠ¤íŠ¸
urls = [
    'https://www.python.org',
    'https://github.com/trending',
    'https://news.naver.com',
]

# í¬ë¡¤ëŸ¬ ì‹¤í–‰
crawler = AdvancedWebCrawler(headless=False)

try:
    # í¬ë¡¤ë§ + ë¶„ì„
    crawler.crawl_multiple(urls, delay=2)
    
    # ê²°ê³¼ ì¶œë ¥
    crawler.print_summary()
    
    # JSON ì €ì¥
    crawler.save_to_json()
    
finally:
    crawler.close()
```

### ì‹¤í–‰ ëª…ë ¹ì–´
```powershell
# í†µí•© í¬ë¡¤ëŸ¬ ì‹¤í–‰
python 8BIT/advanced_crawler.py

# ê°œë³„ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
python 8BIT/genre_classifier.py
python 8BIT/text_summarizer.py
python 8BIT/keyword_extractor.py
```

## ğŸ“Š ìˆ˜ì§‘ë˜ëŠ” ë°ì´í„°

### ê¸°ë³¸ ì •ë³´
- `id`: ê³ ìœ  ID (crawl_20240101123456_1)
- `url`: í˜ì´ì§€ URL
- `domain`: ë„ë©”ì¸ (example.com)
- `title`: í˜ì´ì§€ ì œëª©
- `crawled_at`: ìˆ˜ì§‘ ì‹œê°„ (ISO 8601)

### ë¶„ì„ ê²°ê³¼
- `genre`: ì¥ë¥´ (ê¸°ìˆ /ë‰´ìŠ¤/ë¸”ë¡œê·¸/ì‡¼í•‘/ì˜ìƒ)
- `summary`: 1ì¤„ ìš”ì•½ (ìµœëŒ€ 100ì)
- `keywords`: í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 7ê°œ)

### ìƒì„¸ ë°ì´í„°
- `meta_description`: ë©”íƒ€ ì„¤ëª…
- `paragraphs_count`: ë‹¨ë½ ê°œìˆ˜
- `content_length`: ë³¸ë¬¸ ê¸¸ì´

## ğŸ¯ ì¥ë¥´ ë¶„ë¥˜

5ê°€ì§€ ì¹´í…Œê³ ë¦¬ë¡œ ìë™ ë¶„ë¥˜:

| ì¥ë¥´ | ì˜ˆì‹œ ë„ë©”ì¸ | í‚¤ì›Œë“œ |
|------|------------|--------|
| **ê¸°ìˆ ** | github.com, stackoverflow.com | ê°œë°œ, AI, í”„ë¡œê·¸ë˜ë° |
| **ë‰´ìŠ¤** | news.naver.com, chosun.com | ì†ë³´, ì •ì¹˜, ê²½ì œ |
| **ë¸”ë¡œê·¸** | blog.naver.com, tistory.com | ì¼ìƒ, í›„ê¸°, ì—¬í–‰ |
| **ì‡¼í•‘** | coupang.com, gmarket.co.kr | ê°€ê²©, í• ì¸, ìƒí’ˆ |
| **ì˜ìƒ** | youtube.com, vimeo.com | ë™ì˜ìƒ, êµ¬ë…, ì¬ìƒ |

## ğŸ“ ê°œë³„ ëª¨ë“ˆ ì‚¬ìš©ë²•

### 1. ì¥ë¥´ ë¶„ë¥˜ê¸°
```python
from genre_classifier import GenreClassifier

classifier = GenreClassifier()
genre = classifier.classify(
    url='https://github.com/python',
    title='Python ê³µì‹ ì €ì¥ì†Œ',
    content='íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë° ì–¸ì–´...'
)
print(f"ì¥ë¥´: {genre}")  # ì¶œë ¥: ì¥ë¥´: ê¸°ìˆ 
```

### 2. í…ìŠ¤íŠ¸ ìš”ì•½ê¸°
```python
from text_summarizer import TextSummarizer

summarizer = TextSummarizer()
summary = summarizer.summarize(
    text='ê¸´ ë³¸ë¬¸ í…ìŠ¤íŠ¸...',
    title='í˜ì´ì§€ ì œëª©',
    max_length=100
)
print(f"ìš”ì•½: {summary}")
```

### 3. í‚¤ì›Œë“œ ì¶”ì¶œê¸°
```python
from keyword_extractor import KeywordExtractor

extractor = KeywordExtractor()
keywords = extractor.extract_keywords(
    text='ë³¸ë¬¸ í…ìŠ¤íŠ¸...',
    title='í˜ì´ì§€ ì œëª©',
    max_keywords=7
)
print(f"í‚¤ì›Œë“œ: {', '.join(keywords)}")
```

## ğŸ’¾ ì¶œë ¥ í˜•ì‹ (JSON)

```json
{
  "total_count": 3,
  "crawled_at": "2024-01-01T12:34:56",
  "stats": {
    "total_pages": 3,
    "total_keywords": 18,
    "avg_keywords_per_page": 6.0,
    "genre_distribution": {
      "ê¸°ìˆ ": 2,
      "ë‰´ìŠ¤": 1
    }
  },
  "data": [
    {
      "id": "crawl_20240101123456_1",
      "url": "https://example.com",
      "domain": "example.com",
      "title": "í˜ì´ì§€ ì œëª©",
      "genre": "ê¸°ìˆ ",
      "summary": "OpenAIê°€ ìµœì‹  AI ëª¨ë¸ì„ ê³µê°œí–ˆìœ¼ë©°...",
      "keywords": ["AI", "OpenAI", "ëª¨ë¸", "ê¸°ìˆ "],
      "crawled_at": "2024-01-01T12:34:56"
    }
  ]
}
```

## ğŸ”§ ê³ ê¸‰ ì„¤ì •

### í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ
```python
# ë¸Œë¼ìš°ì € ìˆ¨ê¹€ (ì„œë²„ í™˜ê²½)
crawler = AdvancedWebCrawler(headless=True)

# ë¸Œë¼ìš°ì € í‘œì‹œ (ê°œë°œ/ë””ë²„ê¹…)
crawler = AdvancedWebCrawler(headless=False)
```

### í¬ë¡¤ë§ ì†ë„ ì¡°ì ˆ
```python
# delay: í˜ì´ì§€ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
crawler.crawl_multiple(urls, delay=2)  # 2ì´ˆ ëŒ€ê¸°
crawler.crawl_multiple(urls, delay=5)  # 5ì´ˆ ëŒ€ê¸°
```

### ìš”ì•½ ê¸¸ì´ ì¡°ì ˆ
```python
summary = summarizer.summarize(text, title, max_length=50)   # ì§§ê²Œ
summary = summarizer.summarize(text, title, max_length=100)  # ê¸°ë³¸
summary = summarizer.summarize(text, title, max_length=200)  # ê¸¸ê²Œ
```

### í‚¤ì›Œë“œ ê°œìˆ˜ ì¡°ì ˆ
```python
keywords = extractor.extract_keywords(text, title, max_keywords=3)   # 3ê°œ
keywords = extractor.extract_keywords(text, title, max_keywords=7)   # 7ê°œ (ê¸°ë³¸)
keywords = extractor.extract_keywords(text, title, max_keywords=10)  # 10ê°œ
```

## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
8BIT/
â”œâ”€â”€ advanced_crawler.py       # í†µí•© í¬ë¡¤ëŸ¬ (ë©”ì¸)
â”œâ”€â”€ web_crawler.py             # ê¸°ë³¸ í¬ë¡¤ëŸ¬
â”œâ”€â”€ genre_classifier.py        # ì¥ë¥´ ë¶„ë¥˜
â”œâ”€â”€ text_summarizer.py         # í…ìŠ¤íŠ¸ ìš”ì•½
â”œâ”€â”€ keyword_extractor.py       # í‚¤ì›Œë“œ ì¶”ì¶œ
â”œâ”€â”€ requirements_crawler.txt   # íŒ¨í‚¤ì§€ ëª©ë¡
â””â”€â”€ README_CRAWLER.md          # ì´ íŒŒì¼
```

## âœ… ì™„ë£Œëœ ê¸°ëŠ¥

1. âœ… ë°ì´í„° ìˆ˜ì§‘ (web_crawler.py)
2. âœ… ì¥ë¥´ ë¶„ë¥˜ (genre_classifier.py) - 5ê°€ì§€ ì¹´í…Œê³ ë¦¬
3. âœ… 1ì¤„ ìš”ì•½ ìƒì„± (text_summarizer.py)
4. âœ… ê²€ìƒ‰ì–´ ì¶”ì¶œ (keyword_extractor.py)
5. âœ… í†µí•© í¬ë¡¤ëŸ¬ (advanced_crawler.py)

## ğŸš§ í–¥í›„ ê³„íš

- [ ] Express API ì—°ë™ (ì‹¤ì‹œê°„ í¬ë¡¤ë§ ìš”ì²­)
- [ ] ë°°ì¹˜ ì²˜ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬
- [ ] ì¤‘ë³µ URL í•„í„°ë§
- [ ] ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ (MySQL/PostgreSQL)
- [ ] ì›¹ UI ëŒ€ì‹œë³´ë“œ

## ğŸ› ë¬¸ì œ í•´ê²°

### ChromeDriver ì˜¤ë¥˜
```
selenium.common.exceptions.WebDriverException
```
**í•´ê²°ë°©ë²•**: Chrome ë¸Œë¼ìš°ì €ë¥¼ ìµœì‹  ë²„ì „ìœ¼ë¡œ ì—…ë°ì´íŠ¸

### í•œê¸€ ì¸ì½”ë”© ì˜¤ë¥˜
```
UnicodeEncodeError
```
**í•´ê²°ë°©ë²•**: Python íŒŒì¼ì„ UTF-8ë¡œ ì €ì¥

### ë©”ëª¨ë¦¬ ë¶€ì¡±
**í•´ê²°ë°©ë²•**: í¬ë¡¤ë§ ê°œìˆ˜ë¥¼ ì¤„ì´ê±°ë‚˜ delayë¥¼ ëŠ˜ë¦¼

## ğŸ“ ë¬¸ì˜

ë¬¸ì œê°€ ë°œìƒí•˜ë©´ ì´ìŠˆë¥¼ ë‚¨ê²¨ì£¼ì„¸ìš”.
