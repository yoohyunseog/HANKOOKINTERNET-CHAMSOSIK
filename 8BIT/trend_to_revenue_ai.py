"""
AI ìˆ˜ìµí˜• íŠ¸ë Œë“œ ë¶„ì„ ì‹œìŠ¤í…œ
- íŠ¸ë Œë“œ í‚¤ì›Œë“œ â†’ ì§ˆë¬¸í˜• ë³€í™˜
- YouTube/Naver ê²€ìƒ‰
- í˜ì´ì§€ ìš”ì•½
- ë°ì´í„° ì €ì¥ ë° ì°¸ì†Œì‹.com ì—°ë™
"""

import json
import os
import time
import requests
from datetime import datetime
from pathlib import Path
from urllib.parse import quote, urljoin
from bs4 import BeautifulSoup

class TrendToRevenueAI:
    def __init__(self):
        self.data_dir = "data/naver_creator_trends"
        self.output_dir = "data/revenue_content"
        os.makedirs(self.output_dir, exist_ok=True)
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
    
    def load_trend_data(self):
        """íŠ¸ë Œë“œ ë°ì´í„° ë¡œë“œ"""
        latest_file = os.path.join(self.data_dir, "latest_trend_data.json")
        
        if not os.path.exists(latest_file):
            print("âŒ íŠ¸ë Œë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            print(f"   ë¨¼ì € ì‹¤í–‰: run_naver_creator_analyzer.bat")
            return None
        
        try:
            # UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ ì½ê¸° (ëª…ì‹œì )
            with open(latest_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            trends = data.get('trend_data', [])
            print(f"âœ… íŠ¸ë Œë“œ ë°ì´í„° ë¡œë“œ: {len(trends)}ê°œ í•­ëª©")
            
            # ë¹ˆ í•­ëª© ì œê±°
            valid_trends = [t for t in trends if t.get('title') or t.get('raw_text')]
            if len(valid_trends) < len(trends):
                print(f"âš ï¸  ë¹ˆ í•­ëª© {len(trends) - len(valid_trends)}ê°œ ì œì™¸ë¨")
            
            data['trend_data'] = valid_trends
            return data
        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def keyword_to_question(self, keyword):
        """í‚¤ì›Œë“œ â†’ ì§ˆë¬¸í˜• ë³€í™˜"""
        # ê°„ë‹¨í•œ ì§ˆë¬¸ ìƒì„± ë¡œì§
        questions = [
            f"{keyword}ì— ëŒ€í•´ì„œ ì•Œë ¤ì¤„ë˜?",
            f"{keyword}ì€(ëŠ”) ë¬´ì—‡ì¸ê°€ìš”?",
            f"{keyword}ì— ëŒ€í•´ ìµœì‹  ì •ë³´ê°€ ìˆë‚˜ìš”?",
            f"{keyword} íŠ¸ë Œë“œë¥¼ ì•Œê³  ì‹¶ì–´ìš”.",
            f"{keyword}ì— ëŒ€í•œ ì¸ê¸° ì½˜í…ì¸ ê°€ ìˆì„ê¹Œ?",
        ]
        
        # í•´ì‹œë¥¼ ì‚¬ìš©í•´ì„œ ë™ì¼í•œ í‚¤ì›Œë“œëŠ” ë™ì¼í•œ ì§ˆë¬¸ ë°˜í™˜
        import hashlib
        hash_val = int(hashlib.md5(keyword.encode()).hexdigest(), 16)
        return questions[hash_val % len(questions)]
    
    def search_youtube(self, query):
        """YouTube ê²€ìƒ‰ (URLë§Œ ìƒì„±)"""
        try:
            search_url = f"https://www.youtube.com/results?search_query={quote(query)}"
            # ì‹¤ì œ í˜ì´ì§€ ì ‘ê·¼ ì—†ì´ URLë§Œ ë°˜í™˜
            return {
                'url': search_url,
                'title': f"YouTube - {query} ê²€ìƒ‰ ê²°ê³¼",
                'description': f"'{query}'ì— ëŒ€í•œ YouTube ê²€ìƒ‰ ê²°ê³¼"
            }
        except Exception as e:
            print(f"âš ï¸ YouTube ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return None
    
    def search_naver(self, query):
        """Naver ê²€ìƒ‰ (URLë§Œ ìƒì„±)"""
        try:
            search_url = f"https://search.naver.com/search.naver?query={quote(query)}"
            return {
                'url': search_url,
                'title': f"Naver - {query} ê²€ìƒ‰ ê²°ê³¼",
                'description': f"'{query}'ì— ëŒ€í•œ Naver ê²€ìƒ‰ ê²°ê³¼"
            }
        except Exception as e:
            print(f"âš ï¸ Naver ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return None
    
    def extract_page_summary(self, url):
        """í˜ì´ì§€ ìš”ì•½ ì¶”ì¶œ (ê°„ë‹¨í•œ ë²„ì „)"""
        try:
            response = requests.get(url, headers=self.headers, timeout=5)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë©”íƒ€ ì„¤ëª… ì¶”ì¶œ
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc:
                return meta_desc.get('content', 'ì„¤ëª… ì—†ìŒ')
            
            # ì²« ë¬¸ë‹¨ ì¶”ì¶œ
            paragraphs = soup.find_all('p')
            if paragraphs:
                text = paragraphs[0].get_text().strip()
                return text[:200] if len(text) > 200 else text
            
            return "ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except Exception as e:
            print(f"âš ï¸ í˜ì´ì§€ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return "í˜ì´ì§€ ì ‘ê·¼ ì‹¤íŒ¨"
    
    def simple_summarize(self, text):
        """ê°„ë‹¨í•œ ìš”ì•½ (ì²« ë¬¸ì¥ + í‚¤ì›Œë“œ)"""
        sentences = text.split('.')
        first_sentence = sentences[0].strip() if sentences else text[:100]
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ì‹)
        words = first_sentence.split()
        keywords = [w for w in words if len(w) > 3][:5]
        
        return {
            'first_sentence': first_sentence + '.',
            'keywords': keywords,
            'length': len(text)
        }
    
    def generate_revenue_content(self, trend_data):
        """ìˆ˜ìµí˜• ì½˜í…ì¸  ìƒì„±"""
        revenue_contents = []
        
        trends = trend_data.get('trend_data', [])[:10]  # ìµœëŒ€ 10ê°œ
        
        print(f"\nğŸ“Š {len(trends)}ê°œ íŠ¸ë Œë“œ ì²˜ë¦¬ ì¤‘...\n")
        
        for idx, trend in enumerate(trends, 1):
            try:
                # raw_textì—ì„œ ì²« ë²ˆì§¸ ë¼ì¸ë§Œ ì¶”ì¶œ
                raw_text = trend.get('raw_text', '').strip()
                keyword = raw_text.split('\n')[0].strip()
                
                # ë¹ˆ í‚¤ì›Œë“œê±°ë‚˜ '-'ì¸ ê²½ìš° ê±´ë„ˆë›°ê¸°
                if not keyword or keyword == '-':
                    print(f"[{idx}/{len(trends)}] ê±´ë„ˆëœ€: {repr(keyword[:20])}")
                    continue
                
                # ë„ˆë¬´ ê¸´ í‚¤ì›Œë“œ ìë¥´ê¸°
                keyword = keyword[:50]
                
                print(f"[{idx}/{len(trends)}] ì²˜ë¦¬ ì¤‘: {keyword[:35]}")
                
                # 1. í‚¤ì›Œë“œ â†’ ì§ˆë¬¸ ë³€í™˜
                question = self.keyword_to_question(keyword)
                print(f"  â“ ì§ˆë¬¸: {question}")
                
                # 2. YouTube ê²€ìƒ‰
                yt_result = self.search_youtube(keyword)
                yt_summary = "YouTubeì—ì„œ í•´ë‹¹ ì£¼ì œì˜ ë‹¤ì–‘í•œ ì˜ìƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                
                # 3. Naver ê²€ìƒ‰
                nv_result = self.search_naver(keyword)
                nv_summary = "Naverì—ì„œ ìµœì‹  ì •ë³´ì™€ ë‰´ìŠ¤ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                
                # 4. ì½˜í…ì¸  êµ¬ì„±
                content = {
                    'id': f"trend_{int(time.time() * 1000)}_{idx}",
                    'timestamp': datetime.now().isoformat(),
                    'original_keyword': keyword,
                    'question_form': question,
                    'ai_question': {
                        'text': question,
                        'type': 'trend_inquiry'
                    },
                    'youtube': {
                        'url': yt_result['url'] if yt_result else None,
                        'summary': yt_summary
                    },
                    'naver': {
                        'url': nv_result['url'] if nv_result else None,
                        'summary': nv_summary
                    },
                    'content_summary': {
                        'first_sentence': f"{keyword}ëŠ” ìµœê·¼ ì¸ê¸° ìˆëŠ” íŠ¸ë Œë“œì…ë‹ˆë‹¤.",
                        'key_points': [
                            f"{keyword}ì— ëŒ€í•œ ê´€ì‹¬ë„ ì¦ê°€",
                            "ë‹¤ì–‘í•œ í”Œë«í¼ì—ì„œ ì½˜í…ì¸  ìƒì„± ì¤‘",
                            "ìˆ˜ìµí˜• ì½˜í…ì¸  ê¸°íšŒ ì¡´ì¬"
                        ]
                    },
                    'monetization': {
                        'revenue_keywords': [keyword, question, f"{keyword} ì •ë³´", f"{keyword} ë¶„ì„"],
                        'estimated_views': 0,
                        'estimated_ctr': "ë¯¸ê³„ì‚°"
                    }
                }
                
                revenue_contents.append(content)
                print(f"  âœ… ì™„ë£Œ\n")
                
                time.sleep(0.5)  # API ì œí•œ íšŒí”¼
                
            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜: {e}\n")
                import traceback
                traceback.print_exc()
                continue
        
        return revenue_contents
    
    def save_revenue_data(self, contents):
        """ìˆ˜ìµí˜• ë°ì´í„° ì €ì¥"""
        if not contents:
            print("âŒ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSON ì €ì¥
        json_file = os.path.join(self.output_dir, f"revenue_content_{timestamp}.json")
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump({
                'generation_time': datetime.now().isoformat(),
                'total_items': len(contents),
                'contents': contents
            }, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… JSON ì €ì¥: {json_file}")
        
        # ìµœì‹  íŒŒì¼ë„ ì €ì¥
        latest_file = os.path.join(self.output_dir, "latest_revenue_content.json")
        with open(latest_file, 'w', encoding='utf-8') as f:
            json.dump({
                'generation_time': datetime.now().isoformat(),
                'total_items': len(contents),
                'contents': contents
            }, f, ensure_ascii=False, indent=2)
        
        return True
    
    def display_summary(self, contents):
        """ê²°ê³¼ ìš”ì•½ í‘œì‹œ"""
        print("\n" + "="*60)
        print("ğŸ“ˆ ìˆ˜ìµí˜• ì½˜í…ì¸  ìƒì„± ì™„ë£Œ")
        print("="*60)
        print(f"âœ… ìƒì„±ëœ í•­ëª©: {len(contents)}ê°œ")
        print(f"ğŸ“… ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {self.output_dir}/")
        print("="*60 + "\n")
        
        if contents:
            print("ğŸ“‹ ìƒ˜í”Œ í•­ëª©:")
            sample = contents[0]
            print(f"  ì›ë³¸ í‚¤ì›Œë“œ: {sample.get('original_keyword', 'N/A')}")
            print(f"  ì§ˆë¬¸ í˜•íƒœ: {sample.get('question_form', 'N/A')}")
            print(f"  YouTube URL: {sample.get('youtube', {}).get('url', 'N/A')[:50]}...")
            print(f"  Naver URL: {sample.get('naver', {}).get('url', 'N/A')[:50]}...\n")
    
    def run(self):
        """ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print("\n" + "="*60)
        print("ğŸš€ AI ìˆ˜ìµí˜• íŠ¸ë Œë“œ ì½˜í…ì¸  ìƒì„± ì‹œì‘")
        print("="*60 + "\n")
        
        # 1. íŠ¸ë Œë“œ ë°ì´í„° ë¡œë“œ
        print("[1/4] íŠ¸ë Œë“œ ë°ì´í„° ë¡œë“œ ì¤‘...")
        trend_data = self.load_trend_data()
        if not trend_data:
            return False
        
        # 2. ìˆ˜ìµí˜• ì½˜í…ì¸  ìƒì„±
        print("\n[2/4] ìˆ˜ìµí˜• ì½˜í…ì¸  ìƒì„± ì¤‘...")
        contents = self.generate_revenue_content(trend_data)
        
        if not contents:
            print("âŒ ì½˜í…ì¸  ìƒì„± ì‹¤íŒ¨")
            return False
        
        # 3. ë°ì´í„° ì €ì¥
        print("\n[3/4] ë°ì´í„° ì €ì¥ ì¤‘...")
        if not self.save_revenue_data(contents):
            return False
        
        # 4. ìš”ì•½ í‘œì‹œ
        print("[4/4] ê²°ê³¼ ìš”ì•½")
        self.display_summary(contents)
        
        return True


def main():
    generator = TrendToRevenueAI()
    success = generator.run()
    
    if success:
        print("âœ… ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    else:
        print("âŒ ì‘ì—… ì‹¤íŒ¨")


if __name__ == "__main__":
    main()
