"""
í¬ë¡¤ëŸ¬ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸
ê°„ë‹¨í•œ URL ëª‡ ê°œë¡œ ëª¨ë“  ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

from advanced_crawler import AdvancedWebCrawler

def main():
    # í…ŒìŠ¤íŠ¸í•  URL (í•œêµ­ ì‚¬ì´íŠ¸ ìœ„ì£¼)
    test_urls = [
        # ê¸°ìˆ 
        'https://www.python.org',
        
        # ë‰´ìŠ¤
        'https://www.yonhapnews.co.kr',
        
        # ë¸”ë¡œê·¸ (naverëŠ” ë¡œê·¸ì¸ í•„ìš”í•  ìˆ˜ ìˆìŒ)
        # 'https://blog.naver.com',
    ]
    
    print("=" * 60)
    print("ğŸ§ª í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    print(f"\ní…ŒìŠ¤íŠ¸ URL ê°œìˆ˜: {len(test_urls)}ê°œ")
    print("ë¸Œë¼ìš°ì € ëª¨ë“œ: í‘œì‹œ (ë””ë²„ê¹…ìš©)")
    print("\nì‹œì‘í•˜ë ¤ë©´ Enterë¥¼ ëˆ„ë¥´ì„¸ìš”...")
    input()
    
    crawler = AdvancedWebCrawler(headless=False)
    
    try:
        # í¬ë¡¤ë§ ì‹¤í–‰
        results = crawler.crawl_multiple(test_urls, delay=3)
        
        # ê²°ê³¼ ì¶œë ¥
        crawler.print_summary()
        
        # JSON ì €ì¥
        crawler.save_to_json('data/test_crawled_data.json')
        
        print("\n" + "="*60)
        print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("="*60)
        print("\në‹¤ìŒ ë‹¨ê³„:")
        print("1. data/test_crawled_data.json íŒŒì¼ í™•ì¸")
        print("2. advanced_crawler.pyì—ì„œ urls ìˆ˜ì •")
        print("3. python 8BIT/advanced_crawler.py ì‹¤í–‰")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        print("\në¬¸ì œ í•´ê²°:")
        print("1. Chrome ë¸Œë¼ìš°ì €ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸")
        print("2. pip install -r 8BIT/requirements_crawler.txt ì‹¤í–‰")
        
    finally:
        crawler.close()


if __name__ == '__main__':
    main()
