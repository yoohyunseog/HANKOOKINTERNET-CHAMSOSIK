"""
ë„¤ì´ë²„ ê²€ìƒ‰ ëª¨ë“ˆ
"""

import requests
from urllib.parse import quote
from bs4 import BeautifulSoup
from typing import List, Dict
import re


def search_naver(keyword: str, search_type: str = 'blog') -> str:
    """
    ë„¤ì´ë²„ ê²€ìƒ‰ URL ìƒì„±
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        search_type: ê²€ìƒ‰ ìœ í˜• ('blog', 'news', 'web', 'image', 'video')
    
    Returns:
        ê²€ìƒ‰ URL
    """
    encoded_keyword = quote(keyword)
    
    url_map = {
        'blog': f'https://search.naver.com/search.naver?where=blog&query={encoded_keyword}&sm=tab_opt&sort=0',
        'news': f'https://search.naver.com/search.naver?where=news&query={encoded_keyword}&sm=tab_opt&sort=1&photo=0&field=0&pd=0&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so:dd,p:all,a:all',
        'web': f'https://search.naver.com/search.naver?where=nexearch&query={encoded_keyword}',
        'image': f'https://search.naver.com/search.naver?where=image&query={encoded_keyword}',
        'video': f'https://search.naver.com/search.naver?where=video&query={encoded_keyword}'
    }
    
    return url_map.get(search_type, url_map['blog'])


def get_naver_results(keyword: str, search_type: str = 'blog', limit: int = 5) -> List[Dict]:
    """
    ë„¤ì´ë²„ ê²€ìƒ‰ ê²°ê³¼ í¬ë¡¤ë§
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        search_type: ê²€ìƒ‰ ìœ í˜•
        limit: ê²°ê³¼ ê°œìˆ˜ ì œí•œ
    
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ [{'title': '', 'description': '', 'url': '', 'date': ''}, ...]
    """
    url = search_naver(keyword, search_type)
    results = []
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Referer': 'https://www.naver.com/'
        }
        
        print(f"[ê²€ìƒ‰] {search_type} '{keyword}' ê²€ìƒ‰ ì‹œì‘...")
        print(f"[URL] {url[:80]}...")
        
        response = requests.get(url, headers=headers, timeout=15)
        response.encoding = 'utf-8'
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± (ìš°ì„ ìˆœìœ„)
        if search_type == 'news':
            print(f"[íŒŒì‹±] ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± ì¤‘...")
            # ë„¤ì´ë²„ ë‰´ìŠ¤ëŠ” ì—¬ëŸ¬ ì„ íƒì ì‹œë„
            items = soup.select('.news_area')
            print(f"[ì„ íƒì1 .news_area] {len(items)}ê°œ ë°œê²¬")
            
            if not items:
                items = soup.select('li.bx')
                print(f"[ì„ íƒì2 li.bx] {len(items)}ê°œ ë°œê²¬")
            if not items:
                items = soup.select('.group_news')
                print(f"[ì„ íƒì3 .group_news] {len(items)}ê°œ ë°œê²¬")
            
            items = items[:limit]
            
            for idx, item in enumerate(items, 1):
                try:
                    title_elem = item.select_one('.news_tit') or item.select_one('.tit') or item.select_one('a')
                    desc_elem = item.select_one('.news_dsc') or item.select_one('.dsc')
                    info_elem = item.select_one('.info') or item.select_one('.info_group')
                    
                    title_text = title_elem.get_text(strip=True) if title_elem else ''
                    desc_text = desc_elem.get_text(strip=True) if desc_elem else ''
                    url_link = title_elem.get('href', '') if title_elem else ''
                    date_text = info_elem.get_text(strip=True) if info_elem else ''
                    
                    # HTML íƒœê·¸ ì œê±°
                    title_text = BeautifulSoup(title_text, 'html.parser').get_text()
                    desc_text = BeautifulSoup(desc_text, 'html.parser').get_text()
                    
                    # ë¶€ì¡±í•œ ì„¤ëª… ë³´ì¶©
                    if not desc_text or len(desc_text) < 30:
                        # ë¶€ëª¨ ìš”ì†Œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        parent_text = item.get_text(strip=True)
                        if len(parent_text) > len(title_text):
                            desc_text = parent_text[len(title_text):][:200]
                    
                    result = {
                        'title': title_text[:100] if title_text else '',
                        'description': desc_text[:300] if desc_text else '',
                        'url': url_link,
                        'date': date_text[:50] if date_text else ''
                    }
                    
                    if result['title'] and len(result['title']) > 5:
                        print(f"[{idx}] {result['title'][:40]}... ({len(result['description'])}ê¸€ì)")
                        results.append(result)
                except Exception as e:
                    print(f"[íŒŒì‹± ì˜¤ë¥˜] {e}")
                    continue
        
        # ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±
        elif search_type == 'blog':
            items = soup.select('.view_wrap')
            if not items:
                items = soup.select('.total_wrap')
            
            items = items[:limit]
            
            for item in items:
                title_elem = item.select_one('.title_link') or item.select_one('.link')
                desc_elem = item.select_one('.dsc_link') or item.select_one('.total_dsc')
                date_elem = item.select_one('.sub_time') or item.select_one('.sub_txt')
                
                title_text = title_elem.get_text(strip=True) if title_elem else ''
                desc_text = desc_elem.get_text(strip=True) if desc_elem else ''
                url_link = title_elem.get('href', '') if title_elem else ''
                date_text = date_elem.get_text(strip=True) if date_elem else ''
                
                # HTML íƒœê·¸ ì œê±°
                title_text = BeautifulSoup(title_text, 'html.parser').get_text()
                desc_text = BeautifulSoup(desc_text, 'html.parser').get_text()
                
                result = {
                    'title': title_text,
                    'description': desc_text,
                    'url': url_link,
                    'date': date_text
                }
                
                if result['title']:
                    results.append(result)
        
        # ì¼ë°˜ ì›¹ ê²€ìƒ‰
        else:
            items = soup.select('.total_wrap')[:limit]
            
            for item in items:
                title_elem = item.select_one('.link_tit')
                desc_elem = item.select_one('.total_dsc')
                
                title_text = title_elem.get_text(strip=True) if title_elem else ''
                desc_text = desc_elem.get_text(strip=True) if desc_elem else ''
                url_link = title_elem.get('href', '') if title_elem else ''
                
                # HTML íƒœê·¸ ì œê±°
                title_text = BeautifulSoup(title_text, 'html.parser').get_text()
                desc_text = BeautifulSoup(desc_text, 'html.parser').get_text()
                
                result = {
                    'title': title_text,
                    'description': desc_text,
                    'url': url_link,
                    'date': ''
                }
                
                if result['title']:
                    results.append(result)
        
        # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ì„ íƒì ì‹œë„
        if not results:
            print(f"[ë¶„ì„] í˜ì´ì§€ ì „ì²´ êµ¬ì¡° ë¶„ì„ ì¤‘...")
            all_items = soup.find_all(['a', 'div', 'li'], limit=50)
            for item in all_items:
                text = item.get_text(strip=True)
                if len(text) > 20 and len(text) < 200 and not text.startswith('http'):
                    results.append({
                        'title': text[:100],
                        'description': 'ìƒì„¸ ë‚´ìš©ì„ ë³´ë ¤ë©´ ë§í¬ë¥¼ ë°©ë¬¸í•˜ì„¸ìš”.',
                        'url': item.get('href', url) if item.name == 'a' else url,
                        'date': ''
                    })
                    if len(results) >= limit:
                        break
        
    except Exception as e:
        print(f"[ì˜¤ë¥˜] ë„¤ì´ë²„ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œì—ë„ ê²€ìƒ‰ URLì€ ì œê³µ
        results.append({
            'title': f'"{keyword}" ë„¤ì´ë²„ ê²€ìƒ‰',
            'description': f'{search_type} ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì§ì ‘ í™•ì¸í•˜ì„¸ìš”.',
            'url': url,
            'date': ''
        })
    
    return results


def fetch_page_content(url: str) -> Dict:
    """
    URLì˜ ì‹¤ì œ í˜ì´ì§€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
    
    Args:
        url: í˜ì´ì§€ URL
    
    Returns:
        {'title': '', 'content': '', 'url': ''}
    """
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        response.raise_for_status()
        
        text = response.text
        
        # title ì¶”ì¶œ
        title = ""
        if "<title>" in text:
            try:
                title = text.split("<title>", 1)[1].split("</title>", 1)[0].strip()
            except:
                pass
        
        # body í…ìŠ¤íŠ¸ ì¶”ì¶œ
        # script, style íƒœê·¸ ì œê±°
        text = re.sub(r'<script[^>]*>.*?</script>', '', text, flags=re.DOTALL)
        text = re.sub(r'<style[^>]*>.*?</style>', '', text, flags=re.DOTALL)
        # ëª¨ë“  HTML íƒœê·¸ ì œê±°
        text = re.sub(r'<[^>]+>', '', text)
        # ì—°ì† ê³µë°± ì œê±°
        text = re.sub(r'\s+', ' ', text).strip()
        content = text[:1500]
        
        return {
            'title': title[:100],
            'content': content,
            'url': url
        }
    except Exception as e:
        print(f"[í˜ì´ì§€ ì¡°íšŒ ì˜¤ë¥˜] {url}: {e}")
        return {
            'title': '',
            'content': '',
            'url': url
        }


def get_latest_naver_news(limit: int = 3) -> List[Dict]:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ë©”ì¸ í˜ì´ì§€ì—ì„œ ìµœì‹  ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°
    
    Args:
        limit: ê²°ê³¼ ê°œìˆ˜
    
    Returns:
        ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    results = []
    
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # ë„¤ì´ë²„ ë‰´ìŠ¤ ë©”ì¸ í˜ì´ì§€
        url = "https://news.naver.com"
        print(f"[ìµœì‹ ë‰´ìŠ¤] ë„¤ì´ë²„ ë‰´ìŠ¤ ë©”ì¸ í˜ì´ì§€ ë¡œë“œ ì¤‘...")
        
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = 'utf-8'
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ìµœì‹  ë‰´ìŠ¤ í•­ëª©ë“¤ ì°¾ê¸°
        # ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„
        items = soup.select('.list_body li')  # ì¼ë°˜ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        if not items:
            items = soup.select('article')
        if not items:
            items = soup.select('.list_item')
        
        print(f"[ìµœì‹ ë‰´ìŠ¤] {len(items)}ê°œ í•­ëª© ë°œê²¬")
        
        for idx, item in enumerate(items[:limit * 3], 1):  # ë” ë§ì´ ì°¾ì•„ì„œ ìœ íš¨í•œ ê²ƒ ì¶”ì¶œ
            if len(results) >= limit:
                break
            
            try:
                # ì œëª© ì°¾ê¸°
                title_elem = item.select_one('a.list_title') or item.select_one('a') or item.select_one('.tit')
                if not title_elem:
                    continue
                
                title = title_elem.get_text(strip=True)
                link = title_elem.get('href', '')
                
                # ê°„ë‹¨í•œ ìœ íš¨ì„± ê²€ì‚¬
                if not title or len(title) < 5 or len(title) > 200:
                    continue
                if not link or not link.startswith('https://'):
                    continue
                
                result = {
                    'title': title[:100],
                    'description': title[:50],  # ì œëª©ì˜ ì¼ë¶€ë¥¼ ì„¤ëª…ìœ¼ë¡œ
                    'url': link,
                    'date': ''
                }
                
                print(f"[{len(results)+1}] {title[:50]}...")
                results.append(result)
                
            except Exception as e:
                continue
        
        print(f"[ìµœì‹ ë‰´ìŠ¤] ì´ {len(results)}ê°œ ì¶”ì¶œ")
        
    except Exception as e:
        print(f"[ìµœì‹ ë‰´ìŠ¤] ì˜¤ë¥˜: {e}")
    
    return results


def format_search_results(results: List[Dict]) -> str:
    """
    ê²€ìƒ‰ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
    
    Args:
        results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        í¬ë§·íŒ…ëœ í…ìŠ¤íŠ¸
    """
    if not results:
        return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    output = []
    for i, result in enumerate(results, 1):
        output.append(f"\n[{i}] ğŸ“Œ {result['title']}")
        
        if result['description'] and len(result['description']) > 10:
            output.append(f"    ğŸ’¬ {result['description']}")
        
        if result['date']:
            output.append(f"    ğŸ“… {result['date']}")
        
        if result['url']:
            output.append(f"    ğŸ”— {result['url'][:70]}...")
    
    return '\n'.join(output)
