"""
Selenium ê¸°ë°˜ ë™ì  ì›¹ í¬ë¡¤ë§ - Naver/Bing ê²€ìƒ‰ ê°œì„ 
JavaScript ë Œë”ë§ê¹Œì§€ ì²˜ë¦¬ ê°€ëŠ¥
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
from typing import List, Dict
from urllib.parse import quote
import time
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def get_chrome_driver():
    """Chrome ë“œë¼ì´ë²„ ìƒì„± (í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ)"""
    try:
        options = Options()
        options.add_argument('--headless')  # GUI ì—†ì´ ì‹¤í–‰
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        
        logger.info("âœ… Chrome ë“œë¼ì´ë²„ ì‹œì‘")
        return driver
    except Exception as e:
        logger.error(f"âŒ Chrome ë“œë¼ì´ë²„ ì‹¤íŒ¨: {e}")
        return None


def search_naver_selenium(keyword: str, search_type: str = 'news', limit: int = 5) -> List[Dict]:
    """
    Seleniumì„ ì‚¬ìš©í•œ ë„¤ì´ë²„ ê²€ìƒ‰ (JavaScript ë Œë”ë§ í¬í•¨)
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        search_type: 'news', 'web', 'blog'
        limit: ê²°ê³¼ ê°œìˆ˜
    
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    driver = None
    results = []
    
    try:
        driver = get_chrome_driver()
        if not driver:
            logger.error("ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨")
            return []
        
        # URL êµ¬ì„±
        encoded_keyword = quote(keyword)
        if search_type == 'news':
            url = f'https://search.naver.com/search.naver?where=news&query={encoded_keyword}&sm=tab_opt&sort=1'
        elif search_type == 'blog':
            url = f'https://search.naver.com/search.naver?where=blog&query={encoded_keyword}&sm=tab_opt&sort=0'
        else:
            url = f'https://search.naver.com/search.naver?where=nexearch&query={encoded_keyword}'
        
        logger.info(f"[Selenium] {search_type} '{keyword}' ê²€ìƒ‰ ì‹œì‘")
        logger.info(f"[URL] {url[:80]}...")
        
        # í˜ì´ì§€ ë¡œë“œ
        driver.get(url)
        
        # JavaScript ë Œë”ë§ ëŒ€ê¸° (ìµœëŒ€ 10ì´ˆ)
        try:
            wait = WebDriverWait(driver, 10)
            if search_type == 'news':
                # ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ ëŒ€ê¸°
                wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.news_area, li.bx')))
            else:
                # ì¼ë°˜ ê²€ìƒ‰ ê²°ê³¼ ëŒ€ê¸°
                wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.api_subject_bx, li.bx')))
        except:
            logger.warning("âš ï¸  ë Œë”ë§ ì‹œê°„ ì´ˆê³¼ (ê³„ì† ì§„í–‰)")
        
        # í˜ì´ì§€ ì†ŒìŠ¤ íŒŒì‹±
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        if search_type == 'news':
            logger.info("[íŒŒì‹±] ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± ì¤‘...")
            # ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±
            items = soup.select('div.news_area')
            logger.info(f"[ì„ íƒì1 .news_area] {len(items)}ê°œ ë°œê²¬")
            
            if not items:
                items = soup.select('li.bx')
                logger.info(f"[ì„ íƒì2 li.bx] {len(items)}ê°œ ë°œê²¬")
            
            items = items[:limit]
            
            for idx, item in enumerate(items, 1):
                try:
                    # ì œëª©
                    title_elem = item.select_one('.news_tit, .tit, a')
                    # ì„¤ëª…
                    desc_elem = item.select_one('.news_dsc, .dsc')
                    # ì •ë³´
                    info_elem = item.select_one('.info, .info_group')
                    
                    title = title_elem.get_text(strip=True) if title_elem else ''
                    desc = desc_elem.get_text(strip=True) if desc_elem else ''
                    url_link = title_elem.get('href', '') if title_elem else ''
                    date = info_elem.get_text(strip=True) if info_elem else ''
                    
                    # HTML íƒœê·¸ ì œê±°
                    title = BeautifulSoup(title, 'html.parser').get_text()
                    desc = BeautifulSoup(desc, 'html.parser').get_text()
                    
                    if title and len(title) > 5:
                        results.append({
                            'title': title[:100],
                            'description': desc[:200],
                            'url': url_link if url_link.startswith('http') else f'https://naver.com{url_link}',
                            'date': date[:30],
                            'source': 'Naver News'
                        })
                        logger.info(f"[{idx}] {title[:60]}...")
                
                except Exception as e:
                    logger.debug(f"[íŒŒì‹± ì˜¤ë¥˜] {e}")
                    continue
        
        else:  # web, blog
            logger.info("[íŒŒì‹±] ì›¹/ë¸”ë¡œê·¸ ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± ì¤‘...")
            # ì›¹ ê²€ìƒ‰ ê²°ê³¼
            items = soup.select('div.api_subject_bx, li.bx')
            logger.info(f"[ê²€ìƒ‰ í•­ëª©] {len(items)}ê°œ ë°œê²¬")
            
            items = items[:limit]
            
            for idx, item in enumerate(items, 1):
                try:
                    title_elem = item.select_one('a')
                    
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    url_link = title_elem.get('href', '')
                    
                    # ì„¤ëª… ì¶”ì¶œ
                    desc_elem = item.select_one('div.api_txt_domain, .dsc')
                    desc = desc_elem.get_text(strip=True) if desc_elem else ''
                    
                    if title and len(title) > 5:
                        results.append({
                            'title': title[:100],
                            'description': desc[:200],
                            'url': url_link,
                            'date': '',
                            'source': f'Naver {search_type}'
                        })
                        logger.info(f"[{idx}] {title[:60]}...")
                
                except Exception as e:
                    logger.debug(f"[íŒŒì‹± ì˜¤ë¥˜] {e}")
                    continue
        
        logger.info(f"âœ… ì´ {len(results)}ê°œ ê²°ê³¼ ì¶”ì¶œ")
        return results
    
    except Exception as e:
        logger.error(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []
    
    finally:
        if driver:
            try:
                driver.quit()
                logger.info("âœ… ë“œë¼ì´ë²„ ì¢…ë£Œ")
            except:
                pass


def search_bing_selenium(keyword: str, search_type: str = 'web', limit: int = 5) -> List[Dict]:
    """
    Seleniumì„ ì‚¬ìš©í•œ Bing ê²€ìƒ‰ (JavaScript ë Œë”ë§ í¬í•¨)
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        search_type: 'web', 'news'
        limit: ê²°ê³¼ ê°œìˆ˜
    
    Returns:
        ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    driver = None
    results = []
    
    try:
        driver = get_chrome_driver()
        if not driver:
            logger.error("ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨")
            return []
        
        encoded_keyword = quote(keyword)
        if search_type == 'news':
            url = f'https://www.bing.com/news/search?q={encoded_keyword}'
        else:
            url = f'https://www.bing.com/search?q={encoded_keyword}'
        
        logger.info(f"[Selenium] Bing {search_type} '{keyword}' ê²€ìƒ‰ ì‹œì‘")
        logger.info(f"[URL] {url[:80]}...")
        
        # í˜ì´ì§€ ë¡œë“œ
        driver.get(url)
        
        # ë Œë”ë§ ëŒ€ê¸°
        try:
            wait = WebDriverWait(driver, 10)
            if search_type == 'news':
                wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.news-card')))
            else:
                wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.b_algo')))
        except:
            logger.warning("âš ï¸  ë Œë”ë§ ì‹œê°„ ì´ˆê³¼ (ê³„ì† ì§„í–‰)")
        
        time.sleep(2)  # ì¶”ê°€ ë¡œë”© ì‹œê°„
        
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        
        if search_type == 'news':
            logger.info("[íŒŒì‹±] ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± ì¤‘...")
            items = soup.select('div.news-card')
            logger.info(f"[ê²°ê³¼] {len(items)}ê°œ ë°œê²¬")
            
            items = items[:limit]
            
            for idx, item in enumerate(items, 1):
                try:
                    title_elem = item.select_one('a.title')
                    desc_elem = item.select_one('.snippet')
                    date_elem = item.select_one('.source, span.update-time')
                    
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    desc = desc_elem.get_text(strip=True) if desc_elem else ''
                    url_link = title_elem.get('href', '')
                    date = date_elem.get_text(strip=True) if date_elem else ''
                    
                    if title and len(title) > 5:
                        results.append({
                            'title': title[:100],
                            'description': desc[:200],
                            'url': url_link,
                            'date': date[:30],
                            'source': 'Bing News'
                        })
                        logger.info(f"[{idx}] {title[:60]}...")
                
                except Exception as e:
                    logger.debug(f"[íŒŒì‹± ì˜¤ë¥˜] {e}")
                    continue
        
        else:  # web
            logger.info("[íŒŒì‹±] ì›¹ ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± ì¤‘...")
            items = soup.select('div.b_algo')
            logger.info(f"[ê²°ê³¼] {len(items)}ê°œ ë°œê²¬")
            
            items = items[:limit]
            
            for idx, item in enumerate(items, 1):
                try:
                    title_elem = item.select_one('h2 a')
                    desc_elem = item.select_one('.b_snippet')
                    
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    desc = desc_elem.get_text(strip=True) if desc_elem else ''
                    url_link = title_elem.get('href', '')
                    
                    if title and len(title) > 5:
                        results.append({
                            'title': title[:100],
                            'description': desc[:200],
                            'url': url_link,
                            'date': '',
                            'source': 'Bing'
                        })
                        logger.info(f"[{idx}] {title[:60]}...")
                
                except Exception as e:
                    logger.debug(f"[íŒŒì‹± ì˜¤ë¥˜] {e}")
                    continue
        
        logger.info(f"âœ… ì´ {len(results)}ê°œ ê²°ê³¼ ì¶”ì¶œ")
        return results
    
    except Exception as e:
        logger.error(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []
    
    finally:
        if driver:
            try:
                driver.quit()
                logger.info("âœ… ë“œë¼ì´ë²„ ì¢…ë£Œ")
            except:
                pass


if __name__ == '__main__':
    print("\nğŸ§ª Selenium ê¸°ë°˜ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n")
    
    # Naver ë‰´ìŠ¤ ê²€ìƒ‰
    print("=" * 60)
    print("ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰")
    print("=" * 60)
    results = search_naver_selenium("íŒŒì´ì¬", search_type='news', limit=3)
    for i, r in enumerate(results, 1):
        print(f"\n{i}. {r['title']}")
        print(f"   {r['description'][:100]}")
        print(f"   {r['url'][:80]}")
    
    # Naver ì›¹ ê²€ìƒ‰
    print("\n" + "=" * 60)
    print("ë„¤ì´ë²„ ì›¹ ê²€ìƒ‰")
    print("=" * 60)
    results = search_naver_selenium("ì¸ê³µì§€ëŠ¥", search_type='web', limit=3)
    for i, r in enumerate(results, 1):
        print(f"\n{i}. {r['title']}")
        print(f"   {r['url'][:80]}")
    
    # Bing ì›¹ ê²€ìƒ‰
    print("\n" + "=" * 60)
    print("Bing ì›¹ ê²€ìƒ‰")
    print("=" * 60)
    results = search_bing_selenium("python learning", search_type='web', limit=3)
    for i, r in enumerate(results, 1):
        print(f"\n{i}. {r['title']}")
        print(f"   {r['description'][:100]}")
        print(f"   {r['url'][:80]}")
