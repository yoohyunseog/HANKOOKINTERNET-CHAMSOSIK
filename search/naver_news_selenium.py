"""
Selenium ê¸°ë°˜ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ (ê°œì„ ç‰ˆ)
í˜ì´ì§€ ë Œë”ë§ í›„ ì‹¤ì œ ë‰´ìŠ¤ í•­ëª© íŒŒì‹±
"""

from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
from typing import List, Dict
from urllib.parse import quote
import time
import logging
import re

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def _is_valid_news_title(title: str) -> bool:
    if not title or len(title) < 6:
        return False
    blocked = [
        'ì„œë¹„ìŠ¤ ì˜ì—­', 'NAVER', 'ë©”ë‰´', 'ë°”ë¡œê°€ê¸°', 'ë¡œê·¸ì¸', 'ì „ì²´ë©”ë‰´',
        'ê²€ìƒ‰', 'ê´‘ê³ ', 'ê³µì§€', 'ì–¸ë¡ ì‚¬', 'ë‰´ìŠ¤í™ˆ', 'êµ¬ë…', 'êµ¬ë…í•˜ê¸°'
    ]
    return not any(word in title for word in blocked)


def _is_news_url(url: str) -> bool:
    return url.startswith('http') and 'news.naver.com' in url


def _extract_news_links(soup: BeautifulSoup, limit: int) -> List[Dict]:
    results = []
    seen = set()

    for link in soup.select('a[href]'):
        try:
            href = link.get('href', '')
            title = link.get_text(strip=True)

            if not _is_news_url(href) or not _is_valid_news_title(title):
                continue

            if href in seen:
                continue

            seen.add(href)
            results.append({
                'title': title[:100],
                'description': '',
                'url': href,
                'date': '',
                'source': 'Naver News'
            })

            if len(results) >= limit:
                break
        except Exception:
            continue

    return results


def get_chrome_driver_advanced():
    """ê³ ê¸‰ Chrome ë“œë¼ì´ë²„ ì„¤ì •"""
    try:
        options = Options()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=options)
        driver.set_page_load_timeout(30)
        
        logger.info("âœ… Chrome ë“œë¼ì´ë²„ ì‹œì‘")
        return driver
    except Exception as e:
        logger.error(f"âŒ Chrome ë“œë¼ì´ë²„ ì‹¤íŒ¨: {e}")
        return None


def get_naver_news_by_category(category_id: int = None, limit: int = 10) -> List[Dict]:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ë³„ í¬ë¡¤ë§ (Selenium)
    
    Args:
        category_id: ì¹´í…Œê³ ë¦¬ ID
                    None = ë©”ì¸ ë‰´ìŠ¤
                    100 = ì •ì¹˜
                    101 = ê²½ì œ
                    102 = ì‚¬íšŒ
                    103 = ìƒí™œ/ë¬¸í™”
                    105 = IT/ê³¼í•™
                    etc...
        limit: ìˆ˜ì§‘í•  ë‰´ìŠ¤ ê°œìˆ˜
    
    Returns:
        ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    driver = None
    results = []
    
    try:
        driver = get_chrome_driver_advanced()
        if not driver:
            logger.error("ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨")
            return []
        
        # URL êµ¬ì„±
        if category_id is None:
            url = 'https://news.naver.com/main/main.naver'
        else:
            url = f'https://news.naver.com/section/{category_id}'
        
        logger.info(f"[Selenium] ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘ (ì¹´í…Œê³ ë¦¬: {category_id})")
        logger.info(f"[URL] {url}")
        
        # í˜ì´ì§€ ë¡œë“œ
        driver.get(url)
        time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        
        # JavaScript ë Œë”ë§ ëŒ€ê¸°
        try:
            wait = WebDriverWait(driver, 15)
            wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'a.nclicks')))
        except Exception as e:
            logger.warning(f"âš ï¸  ë Œë”ë§ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼: {e}")
        
        # í˜ì´ì§€ ì†ŒìŠ¤ íŒŒì‹±
        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        
        logger.info("[íŒŒì‹±] ë‰´ìŠ¤ í•­ëª© íŒŒì‹± ì¤‘...")
        
        # ë°©ë²• 1: í˜„ì¬ êµ¬ì¡° ë¶„ì„ - ë‰´ìŠ¤ ë§í¬ ì°¾ê¸°
        logger.info("[ë¶„ì„] í˜ì´ì§€ êµ¬ì¡° ë¶„ì„...")
        
        # ë‰´ìŠ¤ í•­ëª© ì°¾ê¸° (ì—¬ëŸ¬ ê°€ëŠ¥í•œ selector)
        articles = None
        selectors = [
            'div.list_body article',  # ë©”ì¸ í˜ì´ì§€
            'div.list_item',  # ëŒ€ì²´ êµ¬ì¡°
            'li.item',  # ë˜ ë‹¤ë¥¸ êµ¬ì¡°
            'div.article_list li',
        ]
        
        for selector in selectors:
            articles = soup.select(selector)
            if articles:
                logger.info(f"[ë°œê²¬] selector '{selector}' - {len(articles)}ê°œ í•­ëª©")
                break
        
        if not articles:
            logger.warning("[ê²½ê³ ] í‘œì¤€ selectorë¡œ í•­ëª©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë§í¬ ê¸°ë°˜ ì¶”ì¶œ ì‹œë„...")
            results.extend(_extract_news_links(soup, limit))
        
        else:
            # í‘œì¤€ selectorë¡œ ë°œê²¬í•œ ê²½ìš°
            for idx, article in enumerate(articles[:limit], 1):
                try:
                    # ì œëª© ì°¾ê¸°
                    title_elem = article.select_one('a, strong, h2')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    url_link = title_elem.get('href', '')
                    
                    # ì„¤ëª… ì°¾ê¸°
                    desc_elem = article.select_one('.article_content, .news_contents, p')
                    desc = desc_elem.get_text(strip=True)[:150] if desc_elem else ''
                    
                    # ë‚ ì§œ ì°¾ê¸°
                    date_elem = article.select_one('span.date, .time')
                    date = date_elem.get_text(strip=True) if date_elem else ''
                    
                    if _is_valid_news_title(title):
                        results.append({
                            'title': title[:100],
                            'description': desc,
                            'url': url_link if url_link.startswith('http') else f'https://news.naver.com{url_link}',
                            'date': date,
                            'source': 'Naver News (Category)'
                        })
                        logger.info(f"[{idx}] {title[:60]}...")
                
                except Exception as e:
                    logger.debug(f"[íŒŒì‹± ì˜¤ë¥˜] {e}")
                    continue

        if len(results) < limit:
            extra = _extract_news_links(soup, limit)
            for item in extra:
                if len(results) >= limit:
                    break
                if all(item['url'] != existing['url'] for existing in results):
                    results.append(item)
        
        logger.info(f"âœ… ì´ {len(results)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        return results
    
    except Exception as e:
        logger.error(f"âŒ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return []
    
    finally:
        if driver:
            try:
                driver.quit()
                logger.info("âœ… ë“œë¼ì´ë²„ ì¢…ë£Œ")
            except:
                pass


def get_naver_news_search_selenium(keyword: str, limit: int = 10) -> List[Dict]:
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ (Seleniumìœ¼ë¡œ í˜ì´ì§€ ë Œë”ë§ í›„ íŒŒì‹±)
    
    Args:
        keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ
        limit: ê²°ê³¼ ê°œìˆ˜
    
    Returns:
        ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    driver = None
    results = []
    
    try:
        driver = get_chrome_driver_advanced()
        if not driver:
            logger.error("ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨")
            return []
        
        encoded_keyword = quote(keyword)
        url = f'https://search.naver.com/search.naver?where=news&query={encoded_keyword}&sm=tab_opt&sort=1'
        
        logger.info(f"[Selenium] ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘: '{keyword}'")
        logger.info(f"[URL] {url[:90]}")
        
        driver.get(url)
        time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        
        # ë Œë”ë§ ëŒ€ê¸°
        try:
            wait = WebDriverWait(driver, 15)
            wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, 'div.news_area, a.news_tit')))
        except:
            logger.warning("âš ï¸  ë Œë”ë§ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼")
        
        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')
        
        logger.info("[íŒŒì‹±] ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹± ì¤‘...")
        
        # ë‰´ìŠ¤ í•­ëª© ì°¾ê¸°
        items = soup.select('div.news_area')
        logger.info(f"[ë°œê²¬] news_area: {len(items)}ê°œ")
        
        if not items:
            items = soup.select('li.bx')
            logger.info(f"[ë°œê²¬] li.bx: {len(items)}ê°œ")
        
        for idx, item in enumerate(items[:limit], 1):
            try:
                # ì œëª©
                title_elem = item.select_one('a.news_tit, a.tit, a')
                if not title_elem:
                    continue
                
                title = title_elem.get_text(strip=True)
                url_link = title_elem.get('href', '')
                
                # ì„¤ëª…
                desc_elem = item.select_one('div.dsc, span.lede')
                desc = desc_elem.get_text(strip=True)[:150] if desc_elem else ''
                
                # ë‚ ì§œ/ì¶œì²˜
                info_elem = item.select_one('span.info')
                info = info_elem.get_text(strip=True) if info_elem else ''
                
                if _is_valid_news_title(title):
                    results.append({
                        'title': title[:100],
                        'description': desc,
                        'url': url_link if url_link.startswith('http') else url_link,
                        'date': info[:30],
                        'source': 'Naver News Search'
                    })
                    logger.info(f"[{len(results)}] {title[:60]}...")
                    
                    if len(results) >= limit:
                        break
            
            except Exception as e:
                logger.debug(f"[íŒŒì‹± ì˜¤ë¥˜] {e}")
                continue
        
        if len(results) < limit:
            extra = _extract_news_links(soup, limit)
            for item in extra:
                if len(results) >= limit:
                    break
                if all(item['url'] != existing['url'] for existing in results):
                    results.append(item)

        logger.info(f"âœ… ì´ {len(results)}ê°œ ë‰´ìŠ¤ ë°œê²¬")
        return results
    
    except Exception as e:
        logger.error(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return []
    
    finally:
        if driver:
            try:
                driver.quit()
                logger.info("âœ… ë“œë¼ì´ë²„ ì¢…ë£Œ")
            except:
                pass


if __name__ == '__main__':
    print("\n" + "="*70)
    print("ğŸ§ª Selenium ê¸°ë°˜ ë„¤ì´ë²„ ë‰´ìŠ¤ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸")
    print("="*70)
    
    # 1. ë©”ì¸ ë‰´ìŠ¤
    print("\nğŸ“° ë„¤ì´ë²„ ë‰´ìŠ¤ ë©”ì¸ (Selenium)")
    results = get_naver_news_by_category(category_id=None, limit=5)
    print(f"\nâœ… ìˆ˜ì§‘ëœ ë‰´ìŠ¤: {len(results)}ê°œ")
    for i, r in enumerate(results, 1):
        print(f"\n[{i}] {r['title']}")
        if r.get('date'):
            print(f"    ğŸ“… {r['date']}")
    
    # 2. ê²½ì œ ë‰´ìŠ¤
    print("\n\nğŸ“Š ê²½ì œ ë‰´ìŠ¤ (ì¹´í…Œê³ ë¦¬: 101)")
    results = get_naver_news_by_category(category_id=101, limit=5)
    print(f"\nâœ… ìˆ˜ì§‘ëœ ë‰´ìŠ¤: {len(results)}ê°œ")
    for i, r in enumerate(results, 1):
        print(f"\n[{i}] {r['title']}")
    
    # 3. ê²€ìƒ‰ ê²°ê³¼
    print("\n\nğŸ” ê²€ìƒ‰: 'ì£¼ìš” ë‰´ìŠ¤'")
    results = get_naver_news_search_selenium("ì£¼ìš” ë‰´ìŠ¤", limit=5)
    print(f"\nâœ… ìˆ˜ì§‘ëœ ë‰´ìŠ¤: {len(results)}ê°œ")
    for i, r in enumerate(results, 1):
        print(f"\n[{i}] {r['title']}")
        if r.get('description'):
            print(f"    ğŸ“ {r['description'][:80]}")
    
    print("\n" + "="*70 + "\n")
